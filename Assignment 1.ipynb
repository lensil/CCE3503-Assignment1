{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Missing Data and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Non-Predictive Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_unclean = pd.read_csv('CommViolPredUnnormalizedData.csv', na_values=['?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the non-predictive features\n",
    "columns_to_remove = [\n",
    "    'communityname',\n",
    "    'state',\n",
    "    'countyCode',\n",
    "    'communityCode',\n",
    "    'fold',\n",
    "    'Unnamed: 0'\n",
    "]\n",
    "\n",
    "# Removing potential target features\n",
    "columns_to_remove += [\n",
    "        'murders', \n",
    "        'murdPerPop', \n",
    "        'rapes', \n",
    "        'rapesPerPop', \n",
    "        'robberies', \n",
    "        'robbbPerPop', \n",
    "        'assaults', \n",
    "        'burglaries',\n",
    "        'burglPerPop', \n",
    "        'larcenies', \n",
    "        'larcPerPop',\n",
    "        'autoTheft', \n",
    "        'autoTheftPerPop', \n",
    "        'arsons', \n",
    "        'arsonsPerPop',\n",
    "        'ViolentCrimesPerPop', \n",
    "        'nonViolPerPop'\n",
    "    ]\n",
    "\n",
    "\n",
    "df_cleaned = df_unclean.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Features with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the missing data in the dataset is identified. For each feature  with misisng data, the percentage of missing data is calculated to help decide on how to handle the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate missing values statistics\n",
    "total_cells = np.prod(df_cleaned.shape)\n",
    "total_missing = df_cleaned.isnull().sum().sum()\n",
    "        \n",
    "\n",
    "missing_values = df_cleaned.isnull().sum()\n",
    "missing_percentages = (missing_values / len(df_cleaned)) * 100\n",
    "        \n",
    "# Create summary DataFrame\n",
    "missing_info = pd.DataFrame({'Missing Values': missing_values, 'Percentage Missing': missing_percentages})\n",
    "missing_info = missing_info[missing_info['Missing Values'] > 0].sort_values('Percentage Missing', ascending=False)\n",
    "\n",
    "# Display missing values statistics\n",
    "if not missing_info.empty:\n",
    "    print(\"-\" * 50)\n",
    "    for idx, row in missing_info.iterrows():\n",
    "        print(f\"{idx}:\")\n",
    "        print(f\"  Missing values: {row['Missing Values']:,}\")\n",
    "        print(f\"  Percentage missing: {row['Percentage Missing']:.2f}%\")\n",
    "        print(\"-\" * 50)\n",
    "else:\n",
    "    print(\"\\nNo missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features which have 84.51% missing data are removed from the dataset. Removing these features may lead to a loss of information. However, imputing the data when the percentage of missing values is high makes it difficult to reliably impute the missing data and could lead to bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with high percentage of missing values\n",
    "for idx, row in missing_info.iterrows():\n",
    "    if row['Percentage Missing'] > 50:\n",
    "        df_cleaned = df_cleaned.drop(columns=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the target the `assaultPerPop` feature, the rows with missing data are removed from the dataset since there are a small number of rows with missing values. Additonally, since this is the target feature, imputing the missing values could lead to biased results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df_cleaned = df_cleaned.dropna(subset=['assaultPerPop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `OtherPerCap` feature, the missing values are imputed with the median of the feature as there is only one missing value. The median is chosen as there are outliers in the feature and a large standard deviation. Moreover, the difference in the mean and median suggests that the data is skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics for OtherPerCap\n",
    "print(df_cleaned['OtherPerCap'].describe())\n",
    "\n",
    "# Calculate the lower and upper bounds for outliers\n",
    "Q1 = df_cleaned['OtherPerCap'].quantile(0.25)\n",
    "Q3 = df_cleaned['OtherPerCap'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Find outliers\n",
    "outliers = df_cleaned[(df_cleaned['OtherPerCap'] < lower_bound) | \n",
    "                      (df_cleaned['OtherPerCap'] > upper_bound)]['OtherPerCap']\n",
    "\n",
    "print(f\"Number of outliers: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with the median\n",
    "df_cleaned.loc[:, 'OtherPerCap'] = df_cleaned['OtherPerCap'].fillna(df_cleaned['OtherPerCap'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.hist(bins=30, figsize=(40, 35))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above historgrams, a lot of the features contain outliers. Hence, a standard scaler is chosen to normalize the data since it is less sensitive to outliers. A standard scaler changes the data into standard scores which results in the mean of the feature being 0 and the standard deviation being 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import minmaxscaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df_normalized = scaler.fit_transform(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert df_normalized back to DataFrame\n",
    "df_normalized_df = pd.DataFrame(df_normalized, columns=df_cleaned.columns)\n",
    "\n",
    "# Create correlation matrix\n",
    "correlation_matrix = df_normalized_df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create heatmap using seaborn\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f', linewidths=0.5)         # Make the plot square-shaped\n",
    "\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set correlation threshold\n",
    "CORRELATION_THRESHOLD = 0.7  # High correlation threshold\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "highly_correlated = np.where(np.abs(correlation_matrix) > CORRELATION_THRESHOLD)\n",
    "highly_correlated = [(correlation_matrix.index[x], correlation_matrix.columns[y], correlation_matrix.iloc[x, y]) \n",
    "                    for x, y in zip(*highly_correlated) if x != y and x < y]  # Remove self-correlations and duplicates\n",
    "\n",
    "print(\"Highly correlated feature pairs (correlation > 0.7):\")\n",
    "for feat1, feat2, corr in highly_correlated:\n",
    "    print(f\"{feat1} -- {feat2}: {corr:.3f}\")\n",
    "\n",
    "# Keep one feature from each highly correlated pair\n",
    "features_to_drop = set()\n",
    "for feat1, feat2, corr in highly_correlated:\n",
    "    # Keep the feature that has higher correlation with target (assaultPerPop)\n",
    "    corr1 = abs(correlation_matrix.loc[feat1, 'assaultPerPop'])\n",
    "    corr2 = abs(correlation_matrix.loc[feat2, 'assaultPerPop'])\n",
    "    if corr1 < corr2:\n",
    "        features_to_drop.add(feat1)\n",
    "    else:\n",
    "        features_to_drop.add(feat2)\n",
    "\n",
    "# Convert df_normalized back to DataFrame\n",
    "df_normalized_df = pd.DataFrame(df_normalized, columns=df_cleaned.columns)\n",
    "\n",
    "# Drop highly correlated features\n",
    "df_normalized_uncorrelated = df_normalized_df.drop(columns=features_to_drop)\n",
    "print(f\"\\nRemoved {len(features_to_drop)} features\")\n",
    "print(\"Remaining features:\", list(df_normalized_uncorrelated.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split features and target\n",
    "X = df_normalized_uncorrelated.drop('assaultPerPop', axis=1)\n",
    "y = df_normalized_uncorrelated['assaultPerPop']\n",
    "\n",
    "# Split data with 80-20 ratio and fixed random state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    random_state=46\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the MLPRegressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=46)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Compute the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.10f}\")\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.title('MLPRegressor Learning Curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def sequential_forward_selection(X, y, max_features=40, n_splits=5, random_state=46):\n",
    "    # Initialize variables\n",
    "    n_features = X.shape[1]\n",
    "    remaining_features = list(X.columns)\n",
    "    selected_features = []\n",
    "    feature_scores = {}\n",
    "    \n",
    "    # Create K-fold cross validator\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    while len(selected_features) < max_features and len(remaining_features) > 0:\n",
    "        best_score = float('inf')\n",
    "        best_feature = None\n",
    "        \n",
    "        # Try each remaining feature\n",
    "        for feature in remaining_features:\n",
    "            current_features = selected_features + [feature]\n",
    "            scores = []\n",
    "            \n",
    "            # Perform k-fold cross validation\n",
    "            for train_idx, val_idx in kf.split(X):\n",
    "                # Split data\n",
    "                X_train, X_val = X.iloc[train_idx][current_features], X.iloc[val_idx][current_features]\n",
    "                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                # Train model\n",
    "                model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=random_state)\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Make predictions and calculate MSE\n",
    "                y_pred = model.predict(X_val)\n",
    "                mse = mean_squared_error(y_val, y_pred)\n",
    "                scores.append(mse)\n",
    "            \n",
    "            # Calculate average MSE across folds\n",
    "            avg_score = np.mean(scores)\n",
    "            \n",
    "            # Update best feature if current feature performs better\n",
    "            if avg_score < best_score:\n",
    "                best_score = avg_score\n",
    "                best_feature = feature\n",
    "        \n",
    "        # Check if adding the best feature improves the score\n",
    "        if len(feature_scores) > 0 and best_score >= list(feature_scores.values())[-1]:\n",
    "            print(\"No improvement in MSE. Stopping early.\")\n",
    "            break\n",
    "            \n",
    "        # Add best feature to selected features\n",
    "        if best_feature is not None:\n",
    "            selected_features.append(best_feature)\n",
    "            remaining_features.remove(best_feature)\n",
    "            feature_scores[len(selected_features)] = best_score\n",
    "            \n",
    "            print(f\"Selected feature {len(selected_features)}: {best_feature} (MSE: {best_score:.10f})\")\n",
    "    \n",
    "    return selected_features, feature_scores\n",
    "\n",
    "# Run SFS on the normalized data\n",
    "X = df_normalized_uncorrelated.drop('assaultPerPop', axis=1)\n",
    "y = df_normalized_uncorrelated['assaultPerPop']\n",
    "\n",
    "selected_features, feature_scores = sequential_forward_selection(X, y)\n",
    "\n",
    "# Plot the feature selection results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(list(feature_scores.keys()), list(feature_scores.values()), marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Sequential Forward Selection Results')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Train final model with selected features\n",
    "X_selected = X[selected_features]\n",
    "X_train_sfs, X_test_sfs, y_train, y_test = train_test_split(\n",
    "    X_selected, y,\n",
    "    test_size=0.2,\n",
    "    random_state=46\n",
    ")\n",
    "\n",
    "mlp_sfs = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=46)\n",
    "mlp_sfs.fit(X_train_sfs, y_train)\n",
    "\n",
    "# Predict and calculate MSE\n",
    "y_pred_sfs = mlp_sfs.predict(X_test_sfs)\n",
    "mse_sfs = mean_squared_error(y_test, y_pred_sfs)\n",
    "print(f\"\\nFinal MSE with SFS selected features: {mse_sfs:.10f}\")\n",
    "\n",
    "\n",
    "df_normalized_uncorrelated.to_csv('normalized_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def sequential_backward_selection(X, y, min_features=40, n_splits=5, random_state=46):\n",
    "    # Initialize variables\n",
    "    n_features = X.shape[1]\n",
    "    remaining_features = list(X.columns)\n",
    "    feature_scores = {n_features: float('inf')}  # Start with worst possible score\n",
    "    \n",
    "    # Create K-fold cross validator\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Get initial score with all features\n",
    "    scores = []\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=random_state)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        scores.append(mse)\n",
    "    \n",
    "    feature_scores[len(remaining_features)] = np.mean(scores)\n",
    "    print(f\"Initial MSE with all {len(remaining_features)} features: {feature_scores[len(remaining_features)]:.10f}\")\n",
    "    \n",
    "    # Continue removing features until we reach min_features\n",
    "    while len(remaining_features) > min_features:\n",
    "        best_score = float('inf')\n",
    "        worst_feature = None\n",
    "        \n",
    "        # Try removing each remaining feature\n",
    "        for feature in remaining_features:\n",
    "            current_features = [f for f in remaining_features if f != feature]\n",
    "            scores = []\n",
    "            \n",
    "            # Perform k-fold cross validation\n",
    "            for train_idx, val_idx in kf.split(X):\n",
    "                X_train, X_val = X.iloc[train_idx][current_features], X.iloc[val_idx][current_features]\n",
    "                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=random_state)\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                y_pred = model.predict(X_val)\n",
    "                mse = mean_squared_error(y_val, y_pred)\n",
    "                scores.append(mse)\n",
    "            \n",
    "            # Calculate average MSE across folds\n",
    "            avg_score = np.mean(scores)\n",
    "            \n",
    "            # Update worst feature if current removal gives better performance\n",
    "            if avg_score < best_score:\n",
    "                best_score = avg_score\n",
    "                worst_feature = feature\n",
    "        \n",
    "        # Check if removing the worst feature improves the score\n",
    "        if best_score >= feature_scores[len(remaining_features)]:\n",
    "            print(\"No improvement in MSE. Stopping early.\")\n",
    "            break\n",
    "            \n",
    "        # Remove worst feature\n",
    "        if worst_feature is not None:\n",
    "            remaining_features.remove(worst_feature)\n",
    "            feature_scores[len(remaining_features)] = best_score\n",
    "            print(f\"Removed feature (remaining: {len(remaining_features)}): {worst_feature} (MSE: {best_score:.10f})\")\n",
    "    \n",
    "    return remaining_features, feature_scores\n",
    "\n",
    "# Run SBS on the normalized data\n",
    "X = df_normalized_uncorrelated.drop('assaultPerPop', axis=1)\n",
    "y = df_normalized_uncorrelated['assaultPerPop']\n",
    "\n",
    "selected_features, feature_scores = sequential_backward_selection(X, y)\n",
    "\n",
    "# Plot the feature selection results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(list(feature_scores.keys()), list(feature_scores.values()), marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Sequential Backward Selection Results')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Train final model with selected features\n",
    "X_selected = X[selected_features]\n",
    "X_train_sbs, X_test_sbs, y_train, y_test = train_test_split(\n",
    "    X_selected, y,\n",
    "    test_size=0.2,\n",
    "    random_state=46\n",
    ")\n",
    "\n",
    "mlp_sbs = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=46)\n",
    "mlp_sbs.fit(X_train_sbs, y_train)\n",
    "\n",
    "# Predict and calculate MSE\n",
    "y_pred_sbs = mlp_sbs.predict(X_test_sbs)\n",
    "mse_sbs = mean_squared_error(y_test, y_pred_sbs)\n",
    "print(f\"\\nFinal MSE with SBS selected features: {mse_sbs:.10f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
